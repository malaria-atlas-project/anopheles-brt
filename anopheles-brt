import warnings
warnings.simplefilter('ignore')
import os,sys,time
# import threading, Queue
import multiprocessing
import Queue
import matplotlib
matplotlib.use('pdf')
import imp
import anopheles_brt
from anopheles_brt.anopheles_query import Session
from anopheles_brt.upload_raster import upload_raster
import datetime
import thread
import map_utils
import numpy as np
from optparse import OptionParser
import sys
import geojson

p = OptionParser('usage: %prog species1 species2 [options]')
p.add_option('-m','--main',help='Whether to serialize in main process. Defaults to 0.',dest='serialize',type='int')
p.add_option('-e','--eo-expand',help='How much to artificially expand the EO region, in km. Defaults to 0.', dest='eo_expand', type='float')
p.add_option('-p','--raster-path',help='Where the raster files are located', dest='raster_path')
p.add_option('-u','--map1-user',help='Which user to log into map1 as',dest='map1_user')


p.set_defaults(serialize=0)
p.set_defaults(eo_expand=0)
p.set_defaults(raster_path='')
p.set_defaults(map1_user='')

(o, args) = p.parse_args()

os.system('ssh -L 2345:localhost:5432 -fN %s@map1.zoo.ox.ac.uk'%o.map1_user)

suff = imp.get_suffixes()[2]

dblock = multiprocessing.Lock()
memlock = multiprocessing.Lock()
s = Session()
species = dict([sp[::-1] for sp in anopheles_brt.list_species(s)])

err_info = []
def targ(q,s):
    while True:
        try:
            config_filename = q.get(block=False)
        except Queue.Empty:
            break

        # imp.load_module(name, file, pathname, description)
        m = imp.load_module(os.path.splitext(config_filename)[0], file(config_filename), '.', suff)
        upload_rasters(m, o.map1_user)

        layer_names = map(lambda x, p=o.raster_path: os.path.join(p, x), m.layer_names)
        glob_name = os.path.join(o.raster_path, m.glob_name)

        try:

            print 'Process %i starting species %s.'%(multiprocessing.current_process().ident,m.species_name)
            species_tup = (species[m.species_name], m.species_name)
    
            # Will call sites_as_ndarray, gem.n_pseudoabsences and extract_environment.
            dblock.acquire()
            print 'Process %i querying database, extracting environmental layers etc. for species %s.'%(multiprocessing.current_process().ident,m.species_name)
            try:
                fname, pseudoabsences, x, eo, weights = anopheles_brt.sites_and_env(s, species_tup, layer_names, glob_name, m.glob_channels, m.buffer_width/111.32, m.n_pseudoabsences, dblock=dblock, n_pseudopresences=m.n_pseudopresences, eo_expand=o.eo_expand/111.32, pseudoabsence_weight=m.pseudoabsence_weight, pseudopresence_weight=m.pseudopresence_weight, real_presences=m.use_real_presences)
                dblock.release()
                print 'Done'
            except:
                dblock.release()
                cls, inst, tb = sys.exc_info()
                raise cls, inst, tb
    
            # The actual BRT code
            print 'Process %i sending species %s into Leathwick et al\'s BRT code.'%(multiprocessing.current_process().ident,m.species_name)
            result_dirname = anopheles_brt.get_result_dir(config_filename)
            m.brt_opts.update({'result.dir':'"'+result_dirname+'"'})
            brt_res = anopheles_brt.brt(fname, m.species_name, m.brt_opts, weights)
    
            if anopheles_brt.np.random.random() < .001:
                print 'Hi, it\'s process %i. I just wanted to say hello.'%(multiprocessing.current_process().ident)
    
            # Write the requested results out
            anopheles_brt.write_brt_results(brt_res, m.species_name, m.saved_results, config_filename)
    
            # Make an evaluator object
            nice_tree_dict = anopheles_brt.unpack_brt_trees(brt_res, layer_names, glob_name, m.glob_channels)
            intercept = anopheles_brt.unpack_gbm_object(brt_res, 'initF')[0][0]
            be = anopheles_brt.brt_evaluator(nice_tree_dict, intercept)
        
            print 'Process %i running intra-sample diagnostics on species %s'%(multiprocessing.current_process().ident,m.species_name)
            anopheles_brt.trees_to_diagnostics(be, fname, m.species_name, m.n_pseudopresences, m.n_pseudoabsences, config_filename)

            # Make the maps and write them to disk.
            memlock.acquire()
            print 'Process %i generating a predictive map for species %s.'%(multiprocessing.current_process().ident,m.species_name)
            try:
                lon,lat,data = anopheles_brt.trees_to_map(be, m.species_name, layer_names, glob_name, m.glob_channels, (m.llclat, m.llclon, m.urclat, m.urclon), config_filename)
                result_dirname = anopheles_brt.get_result_dir(config_filename)
                map_utils.export_raster(lon,lat,data,'probability-map',result_dirname,'flt')
                del data
                memlock.release()            
                print 'Process %i done generating a predictive map for species %s.'%(multiprocessing.current_process().ident,m.species_name)
            except:
                memlock.release()
                cls, inst, tb = sys.exc_info()
                raise cls, inst, tb

            if m.n_pseudoabsences>0:
                print 'Hi!'
                np.savetxt(os.path.join(result_dirname, 'pseudoabsences.csv'), pseudoabsences, delimiter=',')
            if o.eo_expand > 0:
                file(os.path.join(result_dirname,'extended_eo.json'),'w').write(geojson.dumps(eo))
            if m.n_pseudopresences>0:
                np.savetxt(os.path.join(result_dirname, 'pseudopresences.csv'), x[-m.n_pseudoabsences-m.n_pseudopresences: -m.n_pseudoabsences], delimiter=',')

            print 'Process %i has finished species %s.'%(multiprocessing.current_process().ident,m.species_name)
        except:
            cls, inst, tb = sys.exc_info()
            new_error = cls('Did not produce a map for species %s. Original error:\n\n'%(m.species_name))
            raise cls, inst, tb
    
        if q.empty():
            break

# Set up the queue of species names and the worker threads.
snames = args
q = multiprocessing.Queue(maxsize=len(snames))        
[q.put(sn) for sn in snames]

if o.serialize:
    print 'Serializing in main process'
    targ(q,Session())
else:
    workers = [multiprocessing.Process(target=targ, args=(q, Session())) for i in range(int(os.environ['OMP_NUM_THREADS']))]
    # Dispatch the threads.
    [t.start() for t in workers]
    [t.join() for t in workers]
