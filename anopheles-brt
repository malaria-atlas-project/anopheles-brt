import warnings
warnings.simplefilter('ignore')
import os,sys,time
# import threading, Queue
import multiprocessing
import Queue
import matplotlib
matplotlib.use('pdf')
import imp
import anopheles_brt
from anopheles_query import Session
import datetime
import thread
import map_utils
import numpy as np
from optparse import OptionParser
import sys
import geojson

p = OptionParser('usage: %prog species1 species2 [options]')
p.add_option('-m','--main',help='Whether to serialize in main process. Defaults to 0.',dest='serialize',type='int')
p.add_option('-e','--eo-expand',help='How much to artificially expand the EO region, in km. Defaults to 0.', dest='eo_expand', type='float')


p.set_defaults(serialize=0)
p.set_defaults(eo_expand=0)

(o, args) = p.parse_args()

suff = imp.get_suffixes()[2]

dblock = multiprocessing.Lock()
memlock = multiprocessing.Lock()
s = Session()
species = dict([sp[::-1] for sp in anopheles_brt.list_species(s)])

err_info = []
def targ(q,s):
    while True:
        try:
            config_filename = q.get(block=False)
        except Queue.Empty:
            break

        # imp.load_module(name, file, pathname, description)
        m = imp.load_module(os.path.splitext(config_filename)[0], file(config_filename), '.', suff)
        # from m import layer_names, glob_name, glob_channels, buffer_size, brt_args, bbox, species_name
        exec('from %s import layer_names, glob_name, glob_channels, buffer_width, n_pseudoabsences, n_pseudopresences, pseudopresence_weight, pseudoabsence_weight, use_real_presences, brt_opts, llclat, llclon, urclat, urclon, species_name, saved_results'%os.path.splitext(config_filename)[0])

        try:

            print 'Process %i starting species %s.'%(multiprocessing.current_process().ident,species_name)
            species_tup = (species[species_name], species_name)
    
            # Will call sites_as_ndarray, gen_pseudoabsences and extract_environment.
            dblock.acquire()
            print 'Process %i querying database, extracting environmental layers etc. for species %s.'%(multiprocessing.current_process().ident,species_name)
            try:
                fname, pseudoabsences, x, eo, weights = anopheles_brt.sites_and_env(s, species_tup, layer_names, glob_name, glob_channels, buffer_width/111.32, n_pseudoabsences, dblock=dblock, n_pseudopresences=n_pseudopresences, eo_expand=o.eo_expand/111.32, pseudoabsence_weight=pseudoabsence_weight, pseudopresence_weight=pseudopresence_weight, real_presences=use_real_presences)
                dblock.release()
                print 'Done'
            except:
                dblock.release()
                cls, inst, tb = sys.exc_info()
                raise cls, inst, tb
    
            # The actual BRT code
            print 'Process %i sending species %s into Leathwick et al\'s BRT code.'%(multiprocessing.current_process().ident,species_name)
            result_dirname = anopheles_brt.get_result_dir(config_filename)
            brt_opts.update({'result.dir':'"'+result_dirname+'"'})
            brt_res = anopheles_brt.brt(fname, species_name, brt_opts, weights)
    
            if anopheles_brt.np.random.random() < .001:
                print 'Hi, it\'s process %i. I just wanted to say hello.'%(multiprocessing.current_process().ident)
    
            # Write the requested results out
            anopheles_brt.write_brt_results(brt_res, species_name, saved_results, config_filename)
    
            # Make an evaluator object
            nice_tree_dict = anopheles_brt.unpack_brt_trees(brt_res, layer_names, glob_name, glob_channels)
            intercept = anopheles_brt.unpack_gbm_object(brt_res, 'initF')[0][0]
            be = anopheles_brt.brt_evaluator(nice_tree_dict, intercept)
        
            print 'Process %i running intra-sample diagnostics on species %s'%(multiprocessing.current_process().ident,species_name)
            anopheles_brt.trees_to_diagnostics(be, fname, species_name, n_pseudopresences, n_pseudoabsences, config_filename)

            # Make the maps and write them to disk.
            memlock.acquire()
            print 'Process %i generating a predictive map for species %s.'%(multiprocessing.current_process().ident,species_name)
            try:
                lon,lat,data = anopheles_brt.trees_to_map(be, species_name, layer_names, glob_name, glob_channels, (llclat, llclon, urclat, urclon), config_filename)
                result_dirname = anopheles_brt.get_result_dir(config_filename)
                map_utils.export_raster(lon,lat,data,'probability-map',result_dirname,'flt')
                del data
                memlock.release()            
                print 'Process %i done generating a predictive map for species %s.'%(multiprocessing.current_process().ident,species_name)
            except:
                memlock.release()
                cls, inst, tb = sys.exc_info()
                raise cls, inst, tb

            if n_pseudoabsences>0:
                print 'Hi!'
                np.savetxt(os.path.join(result_dirname, 'pseudoabsences.csv'), pseudoabsences, delimiter=',')
            if o.eo_expand > 0:
                file(os.path.join(result_dirname,'extended_eo.json'),'w').write(geojson.dumps(eo))
            if n_pseudopresences>0:
                np.savetxt(os.path.join(result_dirname, 'pseudopresences.csv'), x[-n_pseudoabsences-n_pseudopresences: -n_pseudoabsences], delimiter=',')

            print 'Process %i has finished species %s.'%(multiprocessing.current_process().ident,species_name)
        except:
            cls, inst, tb = sys.exc_info()
            new_error = cls('Did not produce a map for species %s. Original error:\n\n'%(species_name))
            raise cls, inst, tb
    
        if q.empty():
            break

# Set up the queue of species names and the worker threads.
snames = args
q = multiprocessing.Queue(maxsize=len(snames))        
[q.put(sn) for sn in snames]

if o.serialize:
    print 'Serializing in main process'
    targ(q,Session())
else:
    workers = [multiprocessing.Process(target=targ, args=(q, Session())) for i in range(int(os.environ['OMP_NUM_THREADS']))]
    # Dispatch the threads.
    [t.start() for t in workers]
    [t.join() for t in workers]
